{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-02T12:13:08.025408Z",
     "start_time": "2026-01-02T12:13:02.431523Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3008c76",
   "metadata": {},
   "source": [
    "### Reshaping tensors\n",
    "Demonstrates how `torch.arange` can be reshaped into different 2D views and what effect that has on subsequent operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8c7ab1ac12be69fb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-02T12:13:10.269585Z",
     "start_time": "2026-01-02T12:13:10.259130Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 1],\n",
      "        [2, 3],\n",
      "        [4, 5],\n",
      "        [6, 7],\n",
      "        [8, 9]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.arange(10).view(2, 5)\n",
    "\n",
    "print(a.reshape(5,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81180449",
   "metadata": {},
   "source": [
    "### Inspecting tensor strides\n",
    "Shows how to check memory strides, which reveal how tensors step through memory along each dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "674b765fac3512e5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-02T12:13:11.625654Z",
     "start_time": "2026-01-02T12:13:11.622151Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 1)\n"
     ]
    }
   ],
   "source": [
    "print(a.stride())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5e0fe2c",
   "metadata": {},
   "source": [
    "### Broadcasting example\n",
    "Illustrates how tensors with different shapes broadcast to a common shape before element-wise addition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9885f4bc771ddb58",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-02T12:13:34.077290Z",
     "start_time": "2026-01-02T12:13:34.071452Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[2., 2., 2., 2., 2.],\n",
      "         [2., 2., 2., 2., 2.],\n",
      "         [2., 2., 2., 2., 2.]],\n",
      "\n",
      "        [[2., 2., 2., 2., 2.],\n",
      "         [2., 2., 2., 2., 2.],\n",
      "         [2., 2., 2., 2., 2.]],\n",
      "\n",
      "        [[2., 2., 2., 2., 2.],\n",
      "         [2., 2., 2., 2., 2.],\n",
      "         [2., 2., 2., 2., 2.]],\n",
      "\n",
      "        [[2., 2., 2., 2., 2.],\n",
      "         [2., 2., 2., 2., 2.],\n",
      "         [2., 2., 2., 2., 2.]]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.ones(4,1,5)\n",
    "b = torch.ones(1,3,1)\n",
    "\n",
    "# Broadcasting\n",
    "c = a + b\n",
    "\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d3dfa60",
   "metadata": {},
   "source": [
    "### Setting up autograd variables\n",
    "Create scalar tensors with `requires_grad=True` so PyTorch tracks operations for automatic differentiation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c103e1beba775727",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-02T12:13:44.542075Z",
     "start_time": "2026-01-02T12:13:44.540376Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 3, 5])\n"
     ]
    }
   ],
   "source": [
    "print(c.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "426e8f2e",
   "metadata": {},
   "source": [
    "### Forward pass and scalar loss\n",
    "Multiply the tracked tensors and add a constant to obtain a scalar `loss` on which we can call `backward()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5ebfd1bf3732bae3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-02T12:14:28.886405Z",
     "start_time": "2026-01-02T12:14:28.883470Z"
    }
   },
   "outputs": [],
   "source": [
    "# Initialise tensors with grad\n",
    "x = torch.tensor(2.0, requires_grad=True)\n",
    "y = torch.tensor(3.0, requires_grad=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b0789f0",
   "metadata": {},
   "source": [
    "### Inspecting grad functions and calling backward\n",
    "Print the autograd graph metadata, compute gradients, and show why calling `backward()` twice without `retain_graph=True` triggers an error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3767db0ff6cb75d6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-02T12:14:34.047122Z",
     "start_time": "2026-01-02T12:14:34.044348Z"
    }
   },
   "outputs": [],
   "source": [
    "# Forward Pass\n",
    "\n",
    "z = x * y \n",
    "loss = z + 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5723cdc",
   "metadata": {},
   "source": [
    "### Multiple backward passes with retain_graph\n",
    "Demonstrates how to reuse the same computation graph by retaining it, and how gradients accumulate when `backward()` is called twice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "10e7ee9c8d6c087d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-02T12:15:11.001870Z",
     "start_time": "2026-01-02T12:15:10.950533Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss grad_fn: <AddBackward0 object at 0x105cd29e0>\n",
      "Z grad_fn:    <MulBackward0 object at 0x12fa9cca0>\n",
      "dl/dx: 3.0\n",
      "dl/dy: 2.0\n",
      "CRASH: Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward.\n"
     ]
    }
   ],
   "source": [
    "print(f\"Loss grad_fn: {loss.grad_fn}\")\n",
    "\n",
    "print(f\"Z grad_fn:    {loss.grad_fn.next_functions[0][0]}\")\n",
    "\n",
    "loss.backward()\n",
    "\n",
    "print(f\"dl/dx: {x.grad}\")\n",
    "print(f\"dl/dy: {y.grad}\")\n",
    "\n",
    "try:\n",
    "    loss.backward()\n",
    "except RuntimeError as e:\n",
    "    print(f\"CRASH: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "476723333b31f21b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-02T12:15:50.715664Z",
     "start_time": "2026-01-02T12:15:50.711138Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w is 1.0\n",
      " a is 1.0\n",
      " z is 6.0\n",
      "da/dw: 2.0\n",
      "db/dw: 6.0\n"
     ]
    }
   ],
   "source": [
    "w = torch.tensor(1.0, requires_grad=True)\n",
    "a = w ** 2\n",
    "b = a * 2\n",
    "\n",
    "print(f\"w is {w}\\n a is {a}\\n z is {z}\")\n",
    "\n",
    "a.backward(retain_graph=True)\n",
    "\n",
    "print(f\"da/dw: {w.grad}\")\n",
    "\n",
    "b.backward()\n",
    "\n",
    "print(f\"db/dw: {w.grad}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "612eed80739d7912",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
